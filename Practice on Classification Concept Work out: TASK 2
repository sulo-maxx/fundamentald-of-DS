# ðŸ“˜ Classification Pipeline Script
import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import matplotlib.pyplot as plt

# Step 1: Load dataset (Iris example)
data = load_iris()
X, y = data.data, data.target

# Step 2 + 3: Split into 80/20 train-test set
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Step 4: Define pipelines for each model
pipelines = {
    'KNN': Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())]),
    'SVM': Pipeline([('scaler', StandardScaler()), ('svm', SVC(kernel='rbf', probability=True))]),
    'RandomForest': Pipeline([('scaler', StandardScaler()), ('rf', RandomForestClassifier(random_state=42))]),
}

# Train models
for name, pipe in pipelines.items():
    pipe.fit(X_train, y_train)

# Step 5: Evaluate performance
results = {}
for name, pipe in pipelines.items():
    y_pred = pipe.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    prec, rec, f1, _ = precision_recall_fscore_support(
        y_test, y_pred, average='weighted', zero_division=0
    )
    results[name] = {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1}
    print(f"{name} â–¶ Acc={acc:.3f}, Prec={prec:.3f}, Rec={rec:.3f}, F1={f1:.3f}")

# Step 6: Visualize comparison
model_names = list(results.keys())
accuracies = [results[m]['accuracy'] for m in model_names]
f1_scores  = [results[m]['f1'] for m in model_names]

x = np.arange(len(model_names))
width = 0.35

plt.bar(x - width/2, accuracies, width, label='Accuracy')
plt.bar(x + width/2, f1_scores, width, label='F1 Score')
plt.xticks(x, model_names)
plt.ylabel('Score')
plt.title('Model Performance Comparison')
plt.legend()
plt.show()
